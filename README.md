# Data-Wranggling
The gather, appraise, and clean stages make up the data wrangling process. I am obtaining information for this project from three sources: Udacity offers a "twitter archive enhanced.csv" file for manual download. For programmatic download of the "image-predictions.tsv" file, use the Udacity-provided url. requesting information from the Twitter API that isn't included in the "twitter archive enhanced.csv" file. For instance, saving a tweet's favorite count into a "tweet json.txt" file For this project, I began by manually downloading the "twitter archive enhanced.csv" file that Udacity provided, and then I used code in the Jupyter notebook to programmatically download the "image-predictions.tsv" file. Finally, I applied for a developer account on Twitter, which was then used to query the additional data I needed and stored in the "tweet json.txt" file.

I read all three files into the df twits en, df predictions, and df tweets tables after obtaining all the data I needed for the project. I then moved on to the next step, which involved reviewing the data for both quality and tidiness problems. I found 11 quality problems and 4 tidiness problems that were amply documented in the wrangle act file. I performed visual analyses using both an excel spreadsheet and the dataframe from the Jupyter notebook. I found a number of mistakes in the excel sheet, including the rating numerator values and inaccurate dog names. On the other hand, the programmatic evaluation led me to find inaccurate and inconsistent datatypes in the tweet id and timestamp columns, respectively.

Finally, during the cleaning stage, I copied all three dataframes before moving on to apply the pertinent modifications noted during the evaluation stage.
